# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b64syVfaoMO-RsTf3pF2It3Oj_DU20ap
"""

!git clone https://github.com/Owen-Liuyuxuan/visualDet3D.git



from google.colab import drive
drive.mount('/content/drive')

pwd

!unzip -qq "/content/drive/MyDrive/data_object_calib.zip" -d /content/drive/MyDrive/visualDet3D/visualDet3D/data/kitti/training
#!unzip -qq "/content/drive/MyDrive/data_object_image_2.zip.1" -d /content/drive/MyDrive/visualDet3D/visualDet3D/data/kitti/training
#!unzip -qq "/content/drive/MyDrive/data_object_image_3.zip" -d /content/drive/MyDrive/visualDet3D/visualDet3D/data/kitti/training
#!unzip -qq "/content/drive/MyDrive/data_object_label_2.zip" -d /content/drive/MyDrive/visualDet3D/visualDet3D/data/kitti/training

#!unzip -qq "/content/drive/MyDrive/data_object_calib.zip" -d /content/drive/MyDrive/visualDet3D/visualDet3D/data/kitti/testing
!unzip -qq "/content/drive/MyDrive/data_object_image_2.zip.1" -d /content/drive/MyDrive/visualDet3D/visualDet3D/data/kitti/testing

!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip

!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip

!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip

!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_3.zip

!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip

cd /content/drive/MyDrive

cd /content/drive/MyDrive/visualDet3D

pwd

pip install -r requirement.txt

!pip install coloredlogs

!bash ./make.sh

cd config

pwd

cp Monoflex_example.py $CONFIG_FILE.py

cd ..

pwd

!bash ./launchers/det_precompute.sh config/$CONFIG_FILE.py train

!bash ./launchers/det_precompute.sh config/$CONFIG_FILE.py test

!bash ./launchers/train.sh  config/$CONFIG_FILE.py 0 trains

CHECKPOINT_PATH ='/content/drive/MyDrive/visualDet3D/output/MonoFlex/checkpoint/MonoFlex_latest.pth'

!bash ./launchers/eval.sh /content/drive/MyDrive/visualDet3D/config/Monoflex_example.py 0 $CHECKPOINT_PATH test

!bash ./launchers/eval.sh /content/drive/MyDrive/visualDet3D/config/Monoflex_example.py 0 $CHECKPOINT_PATH validation

import importlib
import os
import copy
import numpy as np
import matplotlib.pyplot as plt
import cv2
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, models, transforms
import torchvision
from visualDet3D.data.kitti.utils import write_result_to_file
from visualDet3D.utils.utils import LossLogger, cfg_from_file
from visualDet3D.networks.utils.registry import DETECTOR_DICT, DATASET_DICT, PIPELINE_DICT
from visualDet3D.networks.heads.anchors import Anchors
from visualDet3D.networks.lib.fast_utils.hill_climbing import post_opt
from visualDet3D.networks.utils import BBox3dProjector, BackProjection
from visualDet3D.utils.utils import convertAlpha2Rot, convertRot2Alpha, draw_3D_box, compound_annotation
import visualDet3D.data.kitti.dataset
from visualDet3D.utils.timer import Timer
from numba import jit
from tqdm import tqdm

cfg = cfg_from_file("/content/drive/MyDrive/visualDet3D/config/Monoflex_example.py")

checkpoint_name = "MonoFlex_latest.pth"

cfg.batch_size=1
split_to_test='validation'

# Define dataset_name
is_test_train = split_to_test == 'training'
if split_to_test == 'training':
    dataset_name = cfg.data.train_dataset
elif split_to_test == 'test':
    dataset_name = cfg.data.test_dataset
else:
    dataset_name = cfg.data.val_dataset

# Make dataset
dataset = DATASET_DICT[dataset_name](
        cfg, split_to_test
        )

# Split train/validation data
if split_to_test=='training':
    dataset_val = DATASET_DICT[cfg.data.val_dataset](
            cfg, 'validation'
            )
    dataset.transform = dataset_val.transform
    dataset.collate_fn = dataset_val.collate_fn

# Build a detector network
detector = DETECTOR_DICT[cfg.detector.name](cfg.detector)
detector = detector.cuda()

# Tensor load by GPU
weight_path = os.path.join(cfg.path.checkpoint_path, checkpoint_name)
state_dict = torch.load(weight_path, map_location='cuda:{}'.format(cfg.trainer.gpu))
new_dict = state_dict.copy()
for key in state_dict:
    if 'focalLoss' in key:
        new_dict.pop(key)

# Load the pre-trained model
detector.load_state_dict(new_dict, strict=False)
detector.eval().cuda()

# testing pipeline
test_func = PIPELINE_DICT[cfg.trainer.test_func]

# Load projector and backprojector
projector = BBox3dProjector().cuda()
backprojector = BackProjection().cuda()

def draw_bbox2d_to_image(image, bboxes2d, color=(255, 0, 255)):
    drawed_image = image.copy()
    for box2d in bboxes2d:
        cv2.rectangle(drawed_image, (int(box2d[0]), int(box2d[1])), (int(box2d[2]), int(box2d[3])), color, 3)
    return drawed_image

index = 0
def corner_homo2bbox(corner_homo):
    """
        corner_homo: [N, 8, 3]
    """
    min_xy  = torch.min(corner_homo[:, :, 0:2], dim=1)[0]
    max_xy  = torch.max(corner_homo[:, :, 0:2], dim=1)[0]
    min_xy[:, 0]  = torch.clamp(min_xy[:, 0], 0, cfg.rgb_shape[1])
    min_xy[:, 1]  = torch.clamp(min_xy[:, 1], 0, cfg.rgb_shape[0])
    max_xy[:, 0]  = torch.clamp(max_xy[:, 0], 0, cfg.rgb_shape[1])
    max_xy[:, 1]  = torch.clamp(max_xy[:, 1], 0, cfg.rgb_shape[0])
    return torch.cat([min_xy, max_xy], dim=1)


# Define a function denormalizing an image
def denorm(image):
    new_image = np.array((image * cfg.data.augmentation.rgb_std +  cfg.data.augmentation.rgb_mean) * 255, dtype=np.uint8)
    return new_image

# Define prediction function
def compute_once(index, is_draw=True, is_test_train=True):
    
    # Load image data
    name = "%06d" % index
    data = dataset[index]
    if isinstance(data['calib'], list):
        P2 = data['calib'][0]
    else:
        P2 = data['calib']
    
    # Image original height
    original_height = data['original_shape'][0]

    # Collate data
    collated_data = dataset.collate_fn([data])

    # Image height
    height = collated_data[0].shape[2]
    scale_2d = (original_height - cfg.data.augmentation.crop_top) / height
    
    # if len(collated_data) > 6:
    #     left_images, right_images, _, _, labels, bbox_3d, _ = collated_data
    # else:
    #     left_images, right_images, _, _, labels, bbox_3d = collated_data
    # image = left_images

    # Split collated data
    left_images, _, labels, _, bbox_3d = collated_data
    image = left_images

    # Deactivates autograd engine
    with torch.no_grad():
        
        # Prediction
        left_images, P2 = collated_data[0], collated_data[1]
        scores, bbox, obj_names = detector([left_images.cuda().float().contiguous(),
                                          P2.cuda().float()])
        
        # Prediction output
        P2 = P2[0]
        bbox_2d = bbox[:, 0:4]
        bbox_3d_state = bbox[:, 4:] #[cx,cy,z,w,h,l,alpha]
        bbox_3d_state_3d = backprojector(bbox_3d_state, P2.cuda()) #[x, y, z, w,h ,l, alpha]
        abs_bbox, bbox_3d_corner_homo, thetas = projector(bbox_3d_state_3d, P2.cuda())

            
    
    # Draw 2D, 3D boxes
    rgb_image = denorm(image[0].cpu().numpy().transpose([1, 2, 0]))
    if len(scores) > 0:
        rgb_image = draw_bbox2d_to_image(rgb_image, bbox_2d.cpu().numpy())
        for box in bbox_3d_corner_homo:
            box = box.cpu().numpy().T
            rgb_image = draw_3D_box(rgb_image, box)
    if is_draw:
        plt.imshow(np.clip(rgb_image, 0, 255))

    return np.clip(rgb_image, 0, 255)

# Commented out IPython magic to ensure Python compatibility.
# Sample 1
# %matplotlib inline
fig = plt.figure(figsize=(16,9))
index += 1
a = compute_once(index, is_test_train=False, is_draw=True)